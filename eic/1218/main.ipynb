{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78555e5-8c64-4039-a867-800b3a8f142e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. mac_addr csv 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e407f4-11ec-4678-ac5e-17490eb73035",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read csv"
    }
   },
   "outputs": [],
   "source": [
    "# csv path\n",
    "csv_path = \"file:/Workspace/EIC_데이터엔지니어/databricks_jira/eic/1218/mac_addr.csv\"\n",
    "\n",
    "# spark dataframe \n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", True)           # 첫 행을 컬럼명으로 사용\n",
    "      .option(\"inferSchema\", True)      # 간단한 경우 자동 스키마 추론\n",
    "      # .option(\"delimiter\", \",\")       # 구분자 기본은 ',' (필요 시 지정)\n",
    "      # .option(\"encoding\", \"utf-8\")    # 인코딩 필요 시 지정\n",
    "      .load(csv_path)\n",
    "     )\n",
    "    \n",
    "# 맥 값 전처리 \n",
    "from pyspark.sql import functions as F\n",
    "df = df.withColumn(\n",
    "    \"mac_addr_2\",\n",
    "    F.concat_ws(\n",
    "        \":\",\n",
    "        F.substring(\"MAC_ADDRESS\", 1, 2),\n",
    "        F.substring(\"MAC_ADDRESS\", 3, 2),\n",
    "        F.substring(\"MAC_ADDRESS\", 5, 2),\n",
    "        F.substring(\"MAC_ADDRESS\", 7, 2),\n",
    "        F.substring(\"MAC_ADDRESS\", 9, 2),\n",
    "        F.substring(\"MAC_ADDRESS\", 11, 2),\n",
    "    )\n",
    ")\n",
    "\n",
    "# 맥 해쉬 처리\n",
    "tv_salt = dbutils.secrets.get('admin', 'salt')\n",
    "\n",
    "df = df.withColumn(\"mac_addr_hashed\", \n",
    "                   F.when(\n",
    "                       F.col(\"mac_addr_2\").isNull() | (F.col(\"mac_addr_2\") == ''), \n",
    "                       None)\\\n",
    "                    .otherwise(F.sha2(F.concat(F.col(\"mac_addr_2\"), F.lit(tv_salt)), 256)))\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "330ec3d9-8b5e-42a8-a321-953db00a38c3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767945136119}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# sdf -> pdf \n",
    "pdf = df.toPandas()\n",
    "pdf.columns = ['_'.join(col.split(' ')) for col in pdf.columns]\n",
    "pdf = pdf[['Production_Date'\n",
    "           , 'Model.Suffix'\n",
    "           , 'SET_ID'\n",
    "           , 'MAC_ADDRESS'\n",
    "           , 'mac_addr_hashed']]\n",
    "pdf = pdf.reset_index(drop=False)\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da3dfc5f-73d0-4bd1-bebc-e9520dc84323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(pdf[['mac_addr_hashed']]).createOrReplaceTempView(\"tmp_mac_addr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6152fb46-88c7-438e-bc16-f08f67d9d113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. activation_date - min(crt_date) 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d884e606-f670-4d31-a7f7-560712da9b8f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767947315611}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_result_1 = spark.sql(f''' \n",
    "    \n",
    "    select raw.mac_addr\n",
    "           , min(raw.crt_date) as min_crt_date \n",
    "           , first(raw.Platform_code) as platform_code \n",
    "           , max(raw.last_chg_date) as max_last_chg_date\n",
    "           , date_format(max(raw.last_chg_date), 'yyyy-MM') as last_chg_date_ym\n",
    "    from   eic_data_ods.tlamp.activation_date raw\n",
    "    inner join tmp_mac_addr tmp on raw.mac_addr = tmp.mac_addr_hashed\n",
    "    group by mac_addr\n",
    "''')\n",
    "display(df_result_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb93f0df-b39c-45e4-8666-32da795607a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_result_1\\\n",
    "    .write.mode('overwrite')\\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .saveAsTable(f\"sandbox.z_eunmi1_ko.temp_1218_1\") # 중간 저장 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3727562a-3c47-4a2c-885d-3ab04b325ca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. 노말로그 값 구하기\n",
    "> 3-1) max_last_chg_date를 yyyy-MM 로 바꾸고, date_ym별 mac_addr 대한 마지막 normal_log 추출 (union 활용) <br/>\n",
    "> 3-2) 단, max_last_chg_date < '2025-12' 이면, 데이터 없음으로 추출 (2년 보관 중)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa06b4ab-ea63-47a9-8f0a-09c2b9b4f758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_date_ym = []\n",
    "df_result_1 = spark.table(\"sandbox.z_eunmi1_ko.temp_1218_1\")\n",
    "for row in df_result_1.select(\"last_chg_date_ym\").distinct().collect():\n",
    "    list_date_ym.append(row.last_chg_date_ym)\n",
    "print(tuple(list_date_ym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83861700-0ba5-4783-8288-064cc18cf54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_result_2 = spark.sql(f'''\n",
    "    select nl.mac_addr, nl.context_name, nl.message_id, nl.normal_log, nl.log_create_time, nl.date_ym\n",
    "    from   eic_data_ods.tlamp.normal_log_webos25 nl\n",
    "    inner join tmp_mac_addr mac on nl.mac_addr = mac.mac_addr_hashed\n",
    "    where  nl.date_ym in {tuple(list_date_ym)}\n",
    "      and  nl.X_device_country = 'TR'\n",
    "''')\n",
    "\n",
    "display(df_result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5713991-8e9b-4e4d-9c9a-1059dbc68cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Window 정의\n",
    "window_spec = Window.partitionBy(\"mac_addr\").orderBy(F.col(\"date_ym\").desc(), \n",
    "                                                     F.col(\"log_create_time\").desc())\n",
    "\n",
    "# rank() 적용\n",
    "df_result_2 = df_result_2.withColumn(\"rn\", F.rank().over(window_spec))\n",
    "\n",
    "# rn = 1 조건 필터링\n",
    "df_result_3 = df_result_2.filter(F.col(\"rn\") == 1).select(\"mac_addr\", \"normal_log\", \"date_ym\", \"log_create_time\")\n",
    "\n",
    "# 결과 확인\n",
    "df_result_3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d3159d-f127-4ab9-a857-3f2dd262addd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_result_3\\\n",
    "    .write.mode('overwrite')\\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .saveAsTable(f\"sandbox.z_eunmi1_ko.temp_1218_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe5f876-6618-4c0c-89bd-558d8afbb4d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd71aaf8-2d57-48b7-a517-da4ed3045482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 조인 하고, 확인해보기 (mac_addr 중복 행 있는지)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8131258919606944,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}